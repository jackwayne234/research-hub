# Defining Information: Order from the Prime Number Structure of the Riemann Zeta Function

**Christopher J. W. Riner**
Independent Researcher
ORCID: 0009-0008-9448-9033
DOI: [10.5281/zenodo.18782869](https://doi.org/10.5281/zenodo.18782869)

**February 2026**

---

## Abstract

Physics invokes "information" in contexts ranging from quantum measurement to black hole evaporation, yet no consensus exists on what information physically *is*. Shannon entropy quantifies surprise; von Neumann entropy quantifies mixedness; Landauer's principle assigns an energy cost; Englert's distinguishability measures which-path knowledge; Wheeler's "it from bit" proposes a program. All provide operational measures. None identify a physical referent. This paper proposes a definition: **information is order — the integer structure generated by the prime numbers through multiplication, as encoded in the Euler product of the Riemann zeta function.** The quantitative measure is the Benford deviation $\delta_B$, which tracks how faithfully a physical system's leading-digit distribution reflects the integer-generated baseline given by Benford's Law. Drawing on prior results connecting $\zeta(s)$ to the Schwarzschild metric, Bose-Einstein condensate phase transitions, and causal set theory, we show that $\delta_B$ tracks quantum-to-classical transitions continuously, that Benford's Law emerges as the least-action distribution under scale invariance, and that the Second Law of Thermodynamics can be reframed as the natural return to Benford conformance. Shannon entropy, Landauer's principle, Englert's duality relation, and Wheeler's program all emerge as consequences of the definition rather than axioms. The proposal is falsifiable: if $\delta_B$ fails to track quantum-to-classical transitions in new experimental systems, the framework fails. We present quantitative evidence from Bose-Einstein condensate simulations, Kretschmann scalar profiles of black hole interiors, and a nine-model quantum gravity comparison.

---

## 1. Introduction

### 1.1 The Temperature Analogy

Before Boltzmann, temperature was a primitive. Physicists could measure it (thermometers), use it in equations (ideal gas law, Carnot efficiency), and predict with it (phase transitions, heat flow). But no one could say what temperature *was*. The thermometer reading was the definition.

Boltzmann changed this by identifying the physical referent:

$$T = \frac{2}{3} \frac{\langle E_{\text{kinetic}} \rangle}{k_B}$$

Temperature became derived — a consequence of molecular motion, not an axiom. The thermometer reading did not change. But now one knew what the thermometer was measuring: mean kinetic energy per degree of freedom.

Information is in the position that temperature occupied before Boltzmann. We can measure it (Shannon entropy), use it in equations (channel capacity, Holevo bound), and predict with it (Landauer erasure, black hole entropy). But we cannot say what it physically *is*. The formula is the definition.

This paper proposes the Boltzmann moment for information.

### 1.2 Where Information Appears

The term "information" pervades modern physics, appearing in at least five distinct foundational contexts:

1. **Quantum mechanics.** "Which-path information destroys interference." The double-slit experiment is described in terms of information acquired by a detector, yet the physical content of that information is never specified.

2. **Black hole physics.** "The black hole information paradox." Hawking radiation appears thermal, implying that the quantum state of infalling matter is lost. Whether this constitutes genuine information loss has driven five decades of research — without agreement on what, precisely, would be lost.

3. **Thermodynamics.** "Erasing information costs energy." Landauer's principle connects information to thermodynamic work, and has been verified experimentally (Berut et al., 2012; Yan et al., 2018). But the principle assigns a *price* to information without identifying the *commodity*.

4. **Holography.** "The boundary encodes all the information of the bulk." The holographic principle and the Bekenstein-Hawking entropy $S = A/(4\ell_P^2)$ count information in units of area — but "information" in this formula means "degrees of freedom," which is circular.

5. **Foundations.** "It from bit" (Wheeler, 1989). The proposal that physical reality derives from information. A program without a mechanism: if everything comes from bits, what do the bits come from?

### 1.3 What a Physical Definition Requires

A satisfactory answer to "what is information?" must meet five criteria:

1. **Identify a physical referent** — what material, field, or structure constitutes information.
2. **Derive the operational definitions** — Shannon entropy, von Neumann entropy, and distinguishability should emerge as consequences, not axioms.
3. **Explain the energy cost** — Landauer's $kT \ln 2$ should follow from the physics, not be imposed as a separate principle.
4. **Apply universally** — the same definition should work for double-slit interference, black hole evaporation, thermodynamics, and quantum computing.
5. **Be falsifiable** — it should predict something that "information as primitive" does not.

### 1.4 Roadmap

Section 2 reviews the five principal definitions currently in use and identifies the common pattern: measures without mechanisms. Section 3 summarizes prior results from the prime framework that this paper builds on. Section 4 presents the proposal. Sections 5–7 develop its mathematical and physical content. Section 8 consolidates quantitative evidence. Sections 9–10 show how existing definitions and higher-prime structure emerge as consequences. Section 11 presents falsification criteria. Section 12 discusses implications and limitations. Section 13 states the conclusions.

---

## 2. Current Definitions

### 2.1 Shannon (1948) — Entropy as Surprise

Shannon defined the information content of a discrete random variable $X$ with probability mass function $\{p_i\}$ as:

$$H(X) = -\sum_i p_i \log_2 p_i \quad [\text{bits}]$$

This quantity measures the average surprise — the expected number of binary questions needed to identify an outcome. A fair coin yields $H = 1$ bit. A loaded coin with $p(\text{heads}) = 0.99$ yields $H \approx 0.08$ bits.

Shannon's definition is mathematically elegant and operationally powerful. It underpins coding theory, channel capacity, and data compression. But it is a property of probability distributions, not of physical systems. It says nothing about what carries the surprise. Switching from base-2 to base-$e$ logarithms changes the units but not the physics — because there is no physics in the formula. Shannon himself warned: "The word 'information' has been given different meanings by various writers. It is likely that at least a number of these will turn out to be useful in certain applications."

**What it lacks:** A physical referent. $H$ is defined over abstract probability spaces. Two systems with identical $\{p_i\}$ carry the same Shannon information regardless of whether the system is a gas, a qubit, or a stock market.

### 2.2 Von Neumann (1932) — Quantum Entropy

The quantum generalization replaces probability distributions with density matrices:

$$S(\rho) = -\text{Tr}(\rho \ln \rho)$$

Pure states have $S = 0$; maximally mixed states of dimension $d$ have $S = \ln d$. Von Neumann entropy quantifies how mixed a quantum state is — how much classical uncertainty remains about the outcome of any measurement.

Like Shannon entropy, this is a property of a mathematical object (the density matrix), not of a physical substance. The formula does not explain why entropy changes when a measurement occurs, or what physically happens during decoherence. It describes the bookkeeping of quantum states without grounding the ledger.

**What it lacks:** An explanation of the measurement process. Why does "looking" change $S$? What physical transaction occurs?

### 2.3 Landauer (1961) — The Energy Cost

Landauer's principle states that erasing one bit of information dissipates at least:

$$W \geq kT \ln 2$$

This was the first claim that information has physical consequences — that it is not merely abstract but participates in thermodynamics. The principle has been verified experimentally in single-atom demonstrations (Yan et al., 2018) and colloidal particle systems (Berut et al., 2012).

Landauer's contribution is profound: information is physical. But the principle assigns a *price* without identifying the *commodity*. Knowing that a gallon of gasoline costs three dollars does not tell you what gasoline is made of. Knowing that erasing a bit costs $kT \ln 2$ does not tell you what a bit is made of.

**What it lacks:** Ontology. The energy cost is established; the substance being priced is not.

### 2.4 Englert (1996) — Distinguishability

In the context of which-path experiments, Englert defined the distinguishability:

$$D = \frac{1}{2} \| \rho_L - \rho_R \|_1$$

where $\rho_L$ and $\rho_R$ are the detector density matrices conditioned on the particle taking the left or right path. The complementarity relation:

$$D^2 + \mathcal{V}^2 \leq 1$$

connects distinguishability to fringe visibility $\mathcal{V}$, formalizing wave-particle duality as a quantitative trade-off.

This is the most operationally precise definition in the quantum measurement context. But $D$ is a distance in Hilbert space — a property of density matrices, not of physical structure. It does not explain what makes two quantum states "distinguishable" in physical terms.

**What it lacks:** A physical mechanism for distinction. $D$ measures the separation between two mathematical objects without specifying what physical substrate supports the separation.

### 2.5 Wheeler (1989) — It from Bit

Wheeler proposed the most radical program:

> "Every it — every particle, every field of force, even the spacetime continuum itself — derives its function, its meaning, its very existence entirely from bits."

This is a philosophical stance, not a physical theory. Wheeler identified the right question — that information might be foundational rather than derived — but provided no mechanism. If reality derives from bits, the immediate question is: what are the bits made of? Wheeler's program is the destination without the route.

**What it lacks:** Everything except the vision. No mechanism, no derivation, no falsifiable prediction.

### 2.6 The Common Pattern

Every definition shares the same structure:

1. Define a mathematical quantity (entropy, distinguishability, energy cost).
2. Show it predicts experimental outcomes.
3. Call it "information."
4. Never say what it physically is.

This is precisely where temperature stood before Boltzmann connected it to molecular kinetic energy. The operational success of these definitions is not in question. What is missing is the physical grounding — the identification of what, in the physical world, these formulas are measuring.

---

## 3. The Prime Framework — Prior Results

This section summarizes results from prior papers that the present proposal builds on. No new derivations are introduced. The goal is to make this paper standalone.

### 3.1 The Metric: $\zeta(s) \times 1/\zeta(s) = 1$

Paper #7 (Riner, 2026) demonstrated that the Schwarzschild metric can be constructed from the Riemann zeta function. The identification is:

$$g_{tt} = -\frac{1}{\zeta(s)}, \qquad g_{rr} = \zeta(s)$$

with the radial mapping $s(r) = 1 + (r/r_s)^3$, where $r_s$ is the Schwarzschild radius. The constraint:

$$g_{tt} \cdot g_{rr} = -\frac{1}{\zeta(s)} \cdot \zeta(s) = -1$$

reproduces the standard Schwarzschild identity $g_{tt} g_{rr} = -1$ exactly. The metric reproduces general relativity to machine precision, including the GPS 38.6 $\mu$s/day time dilation, gravitational redshift, and orbital precession. The Euler product:

$$\zeta(s) = \prod_p \frac{1}{1 - p^{-s}}$$

connects the metric to the prime numbers: spacetime geometry is built from a product over all primes, and curvature arises when this product deviates from unity.

### 3.2 Quantum Statistics: Bosonic $\zeta$, Fermionic $\eta$, Bridge $\varepsilon_B$

Paper #2 (Riner, 2025) established that the Bose-Einstein distribution is the unique quantum statistical distribution satisfying Benford's Law exactly at all temperatures. The key mathematical connection is:

$$\eta(s) = (1 - 2^{1-s}) \cdot \zeta(s)$$

where $\eta(s)$ is the Dirichlet eta function (also known as the alternating zeta function). The bosonic series $\zeta(s) = \sum n^{-s}$ has all positive terms; the fermionic series $\eta(s) = \sum (-1)^{n+1} n^{-s}$ alternates in sign. The bridge factor:

$$\varepsilon_B(s) = |1 - 2^{1-s}|$$

involves only the prime $p = 2$ and governs the separation between bosonic and fermionic statistics. When $\varepsilon_B = 0$ (at $s = 1$), bosonic and fermionic statistics are indistinguishable. When $\varepsilon_B \to 1$ (large $s$), they are maximally separated.

### 3.3 Primes as Causal Set Elements

Paper #6 (Riner, 2025) proposed that prime numbers serve as the elements of a causal set in the sense of Sorkin (2003). In causal set theory, spacetime is fundamentally discrete — a partially ordered set of events. The primes, ordered by magnitude, generate the integers through multiplication; the divisibility relations among integers provide a natural causal structure. The Euler product $\prod_p = \sum_n$ is the statement that the causal set of primes generates the full spacetime of integers.

### 3.4 Benford Conformance Tracks Quantum-to-Classical Transitions

Across Papers #1 and #2, the Benford deviation $\delta_B$ was shown to track quantum-to-classical transitions continuously. In Bose-Einstein condensate simulations, $\delta_B$ is minimal in the fully condensed (quantum) regime and increases as the system transitions to classical gas behavior. In the Kretschmann scalar analysis of black hole interiors, $\delta_B$ maps the radial profile from exterior (classical) through horizon to singularity (where integer structure breaks down). The diagnostic is not a binary gate — it is a continuous spectrum.

---

## 4. The Proposal

### 4.1 Primes: The Foundation

The prime numbers — 2, 3, 5, 7, 11, 13, ... — have no closed-form generating formula. Their distribution among the integers appears random at the local level (the gaps between consecutive primes are irregular and unpredictable). No algorithm can predict the next prime from the previous ones without direct verification. They are the irreducible, patternless atoms of arithmetic.

This is their defining characteristic: **primes are pure disorder.** They carry no pattern, no periodicity, no internal structure that can be compressed or simplified. They are not information. They are the foundation on which information is built.

### 4.2 Integers: The Information

The integers — 1, 2, 3, 4, 5, ... — are the opposite. They form a sequence with total regularity: every integer has a unique successor, a unique predecessor (except 1), a definite position, and a definite relationship to every other integer. The structure is complete, predictable, and infinite.

The Fundamental Theorem of Arithmetic states that every integer greater than 1 factors uniquely into primes. The perfectly disordered primes, through multiplication alone, produce the perfectly ordered integers. No information was added. No external structure was imposed. **Order emerged from the foundation through the single operation of multiplication.**

This is where information lives: in the ordered structure that the primes produce.

### 4.3 The Euler Product as Proof

The Euler product identity:

$$\prod_p \frac{1}{1 - p^{-s}} = \sum_{n=1}^{\infty} \frac{1}{n^s}$$

is the mathematical statement that the product over primes (left side) equals the sum over integers (right side). The left side is the foundation — each factor involves a single prime. The right side is the information — each term involves a single integer. The identity says: **the foundation generates the information.** Every integer on the right is produced by a unique combination of primes on the left.

This is not a metaphor or an analogy. It is an exact mathematical identity, valid for all $\text{Re}(s) > 1$, connecting the disordered foundation to the ordered structure it generates. The Euler product is the mechanism that Wheeler's "it from bit" lacked: the explicit construction of order from elementary, structureless components.

### 4.4 The Measure: Benford Deviation $\delta_B$

The quantitative measure of information content is the Benford deviation $\delta_B$, defined as follows. For a dataset $\{x_1, x_2, \ldots, x_N\}$ of $N$ positive real numbers, extract the leading digit $d_i \in \{1, 2, \ldots, 9\}$ of each $x_i$. The observed frequency of digit $d$ is:

$$f_{\text{obs}}(d) = \frac{1}{N} \sum_{i=1}^{N} \mathbf{1}[d_i = d]$$

Benford's Law predicts the theoretical frequency:

$$f_{\text{Ben}}(d) = \log_{10}\!\left(1 + \frac{1}{d}\right)$$

The Benford deviation is the mean absolute deviation between observed and predicted frequencies:

$$\delta_B = \frac{1}{9} \sum_{d=1}^{9} |f_{\text{obs}}(d) - f_{\text{Ben}}(d)|$$

**Properties of $\delta_B$:**

- $\delta_B = 0$: perfect Benford conformance. The system's leading-digit distribution exactly matches the integer-generated baseline. Maximum order.
- $\delta_B > 0$: deviation from Benford. The integer structure is partially broken or obscured. Order is reduced.
- $\delta_B$ is bounded: $0 \leq \delta_B \leq \delta_{\max}$, where $\delta_{\max}$ corresponds to all data having the same leading digit.
- $\delta_B$ is dimensionless, scale-invariant, and computable from any dataset of positive real numbers.

### 4.5 The Formal Statement

**Definition.** *Information is order: the structure of the integers, generated by the prime numbers through multiplication, as encoded in the Euler product of the Riemann zeta function. The quantitative measure of information content in a physical system is the Benford deviation $\delta_B$, which measures how faithfully the system's internal structure reflects the integer-generated baseline.*

The components of this definition:

| Component | Referent |
|-----------|----------|
| Information | Order — the structure of the integers |
| Foundation | The prime numbers (disorder, irreducible) |
| Mechanism | Multiplication (Euler product: $\prod_p = \sum_n$) |
| Measure | Benford deviation $\delta_B$ |
| Signature | Benford's Law: $P(d) = \log_{10}(1 + 1/d)$ |

### 4.6 Everything in Spacetime Has Information

Anything that exists in spacetime — matter, light, fields, radiation — is ordered. It has structure: a position, a momentum, a mass, a charge. Therefore it has information, and that information can be measured by $\delta_B$.

This follows directly from the metric construction of Section 3.1. The spacetime metric is built from $\zeta(s)$, which is the sum over all integers. Anything that lives in that metric inherits the integer structure. To exist in spacetime is to carry order. There is no physical entity with zero information — existence and order are equivalent within the prime framework.

---

## 5. Benford's Law as the Signature

### 5.1 Why Benford?

Benford's Law states that in naturally occurring datasets spanning several orders of magnitude, the leading digit $d$ appears with probability:

$$P(d) = \log_{10}\!\left(1 + \frac{1}{d}\right)$$

giving $P(1) \approx 30.1\%$, $P(2) \approx 17.6\%$, down to $P(9) \approx 4.6\%$. First observed by Newcomb (1881) and rediscovered by Benford (1938), this distribution is remarkable for three properties:

1. **Scale invariance.** The distribution does not change under multiplication by a constant. If a dataset satisfies Benford's Law in meters, it satisfies it in feet, miles, or nanometers. This is precisely the invariance expected of a distribution generated by the scale-free structure of the integers.

2. **Base invariance.** While commonly stated in base 10, the generalization to base $b$ is $P_b(d) = \log_b(1 + 1/d)$. The mathematical content is independent of the number base — a property of the integers themselves, not of decimal notation.

3. **Multiplicative generation.** Any process that involves multiplication of independent factors — exponential growth, power laws, compound interest, nuclear decay — produces Benford-distributed leading digits as the number of factors increases. This is because multiplication in the integer domain maps to addition in the logarithmic domain, and the Central Limit Theorem drives the sum of independent log-distributed variables toward uniformity on $[0, 1)$, which is exactly the Benford distribution.

These three properties are exactly what one expects of the integer structure generated by primes. Multiplication of primes produces integers. The logarithmic spacing of Benford's Law reflects the logarithmic spacing of prime factorizations.

### 5.2 Derivation from Least Action

The principle of least action states that physical systems follow the path extremizing the action functional:

$$\delta S = 0, \qquad S = \int L \, dt$$

We now show that Benford's Law is the unique distribution satisfying an analogous variational principle: **maximum entropy under the constraint of scale invariance.** The derivation proceeds in four steps.

**Step 1. The significand space.** Every positive real number $x > 0$ can be written as $x = m \times 10^k$ where $m \in [1, 10)$ is the significand and $k \in \mathbb{Z}$. The leading digit is determined entirely by $m$. Define the variable:

$$u \equiv \log_{10}(m) \in [0, 1)$$

This maps the significand to the unit interval. The leading digit $d$ corresponds to $u \in [\log_{10} d, \, \log_{10}(d+1))$.

**Step 2. Scale invariance as translation invariance.** Under the rescaling $x \to cx$ (where $c > 0$), the significand transforms as:

$$u \to u + \log_{10}(c) \pmod{1}$$

That is, scale invariance in $x$-space becomes **translation invariance modulo 1** in $u$-space. The unique probability density on $[0, 1)$ invariant under all translations mod 1 is the uniform density:

$$\rho(u) = 1, \qquad u \in [0, 1)$$

This is a standard result: the Haar measure on the circle group $\mathbb{R}/\mathbb{Z}$ is the Lebesgue measure (Einsiedler & Ward, 2011).

**Step 3. Maximum entropy confirms uniqueness.** Among all densities $\rho(u)$ on $[0, 1)$, the Shannon entropy

$$H[\rho] = -\int_0^1 \rho(u) \ln \rho(u) \, du$$

is maximized when $\rho$ is uniform. This follows from Jensen's inequality: for any normalized $\rho \geq 0$,

$$H[\rho] = -\int_0^1 \rho \ln \rho \, du \leq \ln 1 = 0 \quad \text{(relative to uniform)}$$

with equality if and only if $\rho = 1$ a.e. The scale-invariant distribution is thus simultaneously the maximum-entropy distribution — the configuration that imposes no additional structure beyond the symmetry constraint. This is the Jaynes principle (Jaynes, 1957): the least-biased distribution consistent with known constraints.

**Step 4. Recovery of Benford's Law.** With $\rho(u) = 1$ uniform on $[0, 1)$, the probability that the leading digit equals $d$ is:

$$P(d) = \int_{\log_{10} d}^{\log_{10}(d+1)} 1 \, du = \log_{10}(d+1) - \log_{10}(d) = \log_{10}\!\left(1 + \frac{1}{d}\right)$$

This is Benford's Law. It is not postulated — it is **derived** as the unique distribution satisfying two equivalent conditions: (i) invariance under the symmetry group of the problem (scale transformations), and (ii) maximum entropy subject to that symmetry.

**The Euler product as mechanism.** The derivation above identifies the *what* (Benford is the variational ground state) but not the *why* (why do physical systems converge to it?). The Euler product provides the mechanism. Every integer has a unique prime factorization:

$$n = p_1^{a_1} \cdot p_2^{a_2} \cdots p_k^{a_k}$$

Taking logarithms:

$$\ln n = a_1 \ln p_1 + a_2 \ln p_2 + \cdots + a_k \ln p_k$$

For large $n$, this is a sum of many independent contributions with incommensurate magnitudes (since $\ln p_i / \ln p_j$ is irrational for distinct primes). By equidistribution results on sums of incommensurate frequencies (Weyl, 1916), the fractional part $\{\log_{10} n\}$ becomes uniformly distributed on $[0, 1)$ as $n$ ranges over the integers. The multiplicative structure of integers — encoded in the Euler product $\prod_p (1 - p^{-s})^{-1} = \sum_n n^{-s}$ — generates the uniform significand distribution that the variational principle selects.

**The parallel to physics.** The structure of this derivation mirrors the logic of the principle of least action:

| | Dynamics | Benford |
|---|---|---|
| **Symmetry** | Lorentz invariance | Scale invariance |
| **Variational principle** | $\delta S = 0$ | $\delta H = 0$ (max entropy) |
| **Unique solution** | Equations of motion | $P(d) = \log_{10}(1 + 1/d)$ |
| **Mechanism** | Lagrangian structure | Prime factorization (Euler product) |

In both cases, a symmetry constraint plus a variational principle yields a unique solution, and an underlying structure (the Lagrangian; the primes) provides the mechanism that realizes it. Benford's Law is the variational ground state of leading-digit distributions: the configuration a scale-free system adopts when no additional constraints are imposed — when nature follows the path of least action through the integer structure.

### 5.3 Empirical Universality

Benford conformance has been observed in:

- **Physics:** atomic spectra, nuclear decay half-lives, physical constants (Burke & Kincanon, 1991), quantum phase transitions (Sen & Sen, 2011; Rane et al., 2014), Bose-Einstein condensate occupation numbers (Paper #2).
- **Finance:** stock prices, accounting data, tax returns — the basis of fraud detection in forensic accounting.
- **Earth science:** earthquake magnitudes, river lengths, population sizes.
- **AI validation:** model output distributions, synthetic data detection, reliability testing.

This universality is not coincidental. Benford conformance is universal because the integers are universal. Any system whose structure is generated multiplicatively — which includes nearly all physical systems governed by exponential growth, power laws, or scale-free processes — inherits the Benford signature.

---

## 6. The Metric as Order $\times$ Foundation

The metric construction from Section 3.1 acquires a new interpretation:

$$\underbrace{\zeta(s)}_{\text{ordered face}} \times \underbrace{\frac{1}{\zeta(s)}}_{\text{foundation face}} = 1$$

$$\underbrace{g_{rr}}_{\text{space}} \times \underbrace{(-g_{tt})}_{\text{time}} = 1$$

The zeta function $\zeta(s) = \sum_n n^{-s}$ is the sum over all integers — the totality of order, weighted by the parameter $s$. Its inverse $1/\zeta(s)$, expressed via the Möbius function as $\sum_n \mu(n) n^{-s}$, encodes the prime foundation directly through $\mu(n)$, which detects the prime factorization structure of each integer.

Their product is the multiplicative identity. Information and its foundation are inseparable. Neither exists without the other. The metric *is* the statement that order (integers/time) and foundation (primes/space) coexist at every point in spacetime.

The parameter $s = 1 + (r/r_s)^3$ controls the balance:

| Regime | $s$ | $\zeta(s)$ | Physical character |
|--------|-----|-----------|-------------------|
| Far from mass | $s \gg 1$ | $\zeta \approx 1$ | Flat space; integer structure stable; classical order dominant |
| Near horizon | $s \to 1^+$ | $\zeta \to \infty$ | Euler product diverges; integer structure breaking |
| At horizon | $s = 1$ | $\zeta$ diverges | Domain boundary; prime structure ends |

Far from a mass (large $s$), the integer structure dominates — the system is ordered, quantum effects are small, and the Euler product converges rapidly. Near the horizon ($s \to 1$), the Euler product diverges — the integers lose their grip, and the ordered structure dissolves. The metric encodes, at every spacetime point, how much order the prime foundation currently supports.

---

## 7. Entropy as the Return to Benford

### 7.1 The Second Law Reframed

The Second Law of Thermodynamics states that entropy increases in isolated systems. In the standard formulation, entropy is $S = k_B \ln \Omega$, where $\Omega$ is the number of accessible microstates. "Entropy increases" means the system explores more microstates over time. But "disorder increases" has never been grounded as a physical process.

In the present framework, the Second Law has a concrete restatement:

$$\delta_B \to 0 \quad \text{over time, via least action}$$

A system naturally relaxes toward the Benford distribution — the least-action configuration where no prime is favored over any other. This is what entropy *is*: the drive back to the natural integer structure.

Building anything — a crystal, an organism, a machine — means moving *away* from Benford. It means imposing structure on the prime foundation, favoring certain configurations, breaking the scale invariance. This costs energy, because it fights the principle of least action:

| Process | $\delta_B$ direction | Energy |
|---------|---------------------|--------|
| Building structure | $\delta_B$ increases | Costs energy (fights least action) |
| Entropy / relaxation | $\delta_B$ decreases | Releases energy (follows least action) |
| Equilibrium | $\delta_B \to 0$ | Benford conformance — the natural state |

Heat death — the thermal equilibrium of the universe — is Benford conformance: everything relaxed back to the foundation, no structure remaining, no prime favored over any other.

### 7.2 Landauer's $kT \ln 2$ from the $p = 2$ Euler Factor

The energy cost of erasing one bit is $W \geq kT \ln 2$. The factor $\ln 2$ appears throughout information theory as the conversion between bits and nats. In the prime framework, we note that:

$$\ln 2 = -\ln(1 - 2^{-1})$$

which is the logarithm of the $p = 2$ factor in the Euler product: $\ln(1 - p^{-s})^{-1}\big|_{p=2,\, s=1} = -\ln(1 - 2^{-1}) = \ln 2$. The full Euler product diverges at $s = 1$, but the contribution of the single factor $p = 2$ is finite and equals exactly $\ln 2$.

This suggests that Landauer's energy cost is the thermodynamic price of one binary constraint on the prime foundation — the energy required to activate the simplest prime ($p = 2$) and impose one even/odd distinction on the system. Erasing that distinction releases $kT \ln 2$ back to the thermal reservoir.

**Caveat:** This is a suggestive sketch, not a derivation. A rigorous proof would need to connect the Euler product structure to the partition function of the thermal reservoir and derive the factor $kT$ from the statistical mechanics of the prime foundation. This remains an open problem.

### 7.3 Macro vs. Micro

This picture has an important subtlety. Individual components of a system can and do deviate from Benford — they have mass, structure, constraints. A single atom in a crystal is highly constrained. A single transaction in a market has a specific price. Individual deviations are expected and necessary: structure requires them.

But a system *works* — is natural, functional, stable — when the macro-level statistics are Benford-like. The aggregate, not the components, carries the Benford signature:

- **A healthy financial market:** each transaction has specific structure, but the aggregate of all transactions follows Benford. When someone fabricates data, the macro-level Benford conformance breaks — and forensic accountants detect it.
- **A Bose-Einstein condensate:** each atom is in the ground state (maximally constrained), but the condensate as a whole is maximally Benford-conformant. The information is a macro-level property.
- **A reliable AI model:** individual outputs may be constrained by the prompt, but the distribution of outputs over many queries should conform to Benford when the underlying phenomenon does. Deviation signals hallucination or systematic error.

This is why $\delta_B$ is diagnostic: it tests whether the macro-level order is natural. When the aggregate deviates from Benford, the system is being driven by artificial constraints rather than the natural integer structure.

---

## 8. Quantitative Evidence

### 8.1 Bose-Einstein Condensate (BEC) Phase Transitions

Paper #2 simulated 5,000 energy levels at various temperatures using Bose-Einstein and Fermi-Dirac occupation numbers. The Benford deviation $\delta_B$ was computed for each ensemble. Key results:

**Table 1.** Benford deviation by statistics type and temperature.

| $T$ | $\delta_B$ (boson) | $\delta_B$ (fermion) | Ratio |
|-----|-------------------|---------------------|-------|
| 0.5 | 0.0053 | 0.0083 | 1.56$\times$ |
| 1.0 | 0.0111 | 0.0309 | 2.79$\times$ |
| 2.0 | 0.0127 | 0.0509 | 4.02$\times$ |
| 100.0 | 0.2343 | 1.5563 | 6.64$\times$ |

*Note: $\delta_B$ values in this table use the sum of absolute deviations $\sum_{d=1}^{9} |f_{\text{obs}}(d) - f_{\text{Ben}}(d)|$ (i.e., $9 \times \delta_B$ as defined in Section 4.4), matching the original simulation output. Values above 0.2 reflect this unnormalized convention; the qualitative ordering and ratios are unaffected.*

At all temperatures above $T = 0.1$, fermionic occupation numbers deviate from Benford more than bosonic ones. The ratio increases with temperature, reaching $6.64\times$ at $T = 100$. This is consistent with the proposal: bosonic (ordered, wavelike) systems conform to Benford; fermionic (massive, particlelike) systems deviate.

**Table 2.** Coupling simulation: 3,000 bosonic modes mixed with fermionic modes at varying coupling strength $\alpha$.

| $\alpha$ (coupling) | $\delta_B$ | Distinguishability $D$ | Visibility $\mathcal{V}$ |
|--------------------|-----------|----------------------|------------------------|
| 0.00 (no detector) | 0.018 | 0.00 | 1.00 |
| 0.10 | 0.033 | 0.20 | 0.98 |
| 0.30 | 0.040 | 0.60 | 0.80 |
| 0.50 (full detector) | 0.052 | 1.00 | 0.00 |

$\delta_B$ increases from 0.018 to 0.052 (189%) from no-detector to full-detector. The transition is continuous — no discontinuity at any coupling strength. All correlations are in the predicted direction: $r(\delta_B, D) = +0.36$, $r(\delta_B, \mathcal{V}) = -0.27$, $r(\delta_B, \alpha) = +0.25$. These correlations are modest in magnitude, reflecting the limitations of the simulation design (3,000 modes, single temperature, discrete occupation numbers producing statistical noise in leading-digit counts). We expect that correlations would tighten with larger ensembles ($N > 10^5$) and multi-temperature sweeps. An independent experimental test — computing $\delta_B$ from measured occupation numbers across a BEC phase transition — would provide a stronger and less model-dependent check.

### 8.2 Kretschmann Scalar — Radial $\delta_B$ Profile

The Kretschmann scalar $K = R_{\mu\nu\rho\sigma} R^{\mu\nu\rho\sigma}$ was computed for the prime-modified Schwarzschild metric at radii from $r = 10 \, r_s$ (exterior) to $r = 0.01 \, r_s$ (deep interior). At each radius, $\delta_B$ was computed from the metric coefficients.

**Table 3.** Benford deviation and bridge factor across the black hole interior.

| $r/r_s$ | $s(r)$ | $\varepsilon_B$ | $\delta_B$ | $\delta_B < \varepsilon_B$? | Character |
|---------|--------|----------------|-----------|---------------------------|-----------|
| 10.0 | 3.65 | 0.841 | 0.028 | Yes | Quantum |
| 2.0 | 1.73 | 0.396 | 0.003 | Yes | Quantum |
| 1.1 | 1.05 | 0.035 | 0.002 | Yes | Most quantum |
| 1.001 | 1.001 | 0.0007 | 0.004 | **No** | Crossing point |
| 0.5 | — $^*$ | 0.414 | 0.005 | Yes | Quantum |
| 0.1 | — $^*$ | 0.866 | 0.017 | Yes | Most massive |
| 0.01 | — $^*$ | 0.986 | 0.015 | Yes | Recovering |

$^*$ For $r < r_s$, the mapping $s(r)$ requires analytic continuation below $s = 1$; $\varepsilon_B$ values at these radii are computed from the continued metric coefficients directly.

The key finding: $\delta_B$ remains below the bridge threshold $\varepsilon_B$ at every radius except $r \approx 1.001 \, r_s$ — the near-horizon crossing point. This means the Benford diagnostic validates the metric at every radius, certifying that curvature probes (Kretschmann scalar, Ricci decomposition, Weyl tensor) are meaningful throughout the interior.

The geometry becomes most massive ($\delta_B$ peaks) around $r \approx 0.1 \, r_s$, then **recovers toward quantum character** at smaller radii. The black hole interior is not a monotonic collapse — it is a smooth gradient with a maximum-mass zone.

### 8.3 Nine Quantum Gravity Models — The Benford Gate

Nine quantum gravity models were tested for Benford conformance from $r = 10 \, r_s$ to $r = 10^{-4} \, r_s$. All nine pass the Benford gate ($\delta_B < \varepsilon_B$) at every radius.

**Table 4.** Kretschmann scalar $K$ at $r = 10^{-4} \, r_s$ for resolving vs. non-resolving models.

| Model | $K$ at $r = 10^{-4} r_s$ | Singularity behavior |
|-------|--------------------------|---------------------|
| Standard GR | $1.20 \times 10^{25}$ | Divergent ($K \sim r^{-6}$) |
| Asymptotic Safety | $3.83 \times 10^{16}$ | **Bounded** (plateaus) |
| Causal Sets | $4.00 \times 10^{16}$ | **Bounded** (volume floor) |
| Loop QG | $4.68 \times 10^{34}$ | Amplified (bounce not yet active) |
| Non-Comm. Geometry | $1.11 \times 10^{24}$ | Standard divergence |
| String (GUP) | $1.25 \times 10^{24}$ | Standard divergence |
| CDT | $1.25 \times 10^{24}$ | Standard divergence |
| Twistor Theory | $1.20 \times 10^{25}$ | Standard divergence |
| Group Field Theory | $1.18 \times 10^{27}$ | Amplified |
| Emergent Gravity | $1.20 \times 10^{25}$ | Standard divergence |

The models cluster into three tiers: **resolving** (Asymptotic Safety, Causal Sets — $K$ plateaus at $\sim 4 \times 10^{16}$), **standard** (GR, Twistor, Emergent, Non-Comm., String, CDT — $K \sim 10^{24}$–$10^{25}$), and **amplifying** (Loop QG, Group Field Theory — $K > 10^{27}$). Crucially, all nine maintain Benford conformance even at extreme curvature. The Benford diagnostic is robust to curvature spanning 30 orders of magnitude.

---

## 9. Operational Definitions as Consequences

If information is order — the integer structure built on the prime foundation — then the existing operational definitions become *consequences* of this definition, not axioms.

### 9.1 Shannon Entropy

Shannon's $H = -\sum p_i \log_2 p_i$ measures how far a probability distribution deviates from the structure that would be present if the system were fully ordered. Maximum entropy ($H = \log_2 N$ for $N$ equally likely outcomes) corresponds to maximum deviation from the integer-generated baseline — uniform randomness that respects no multiplicative structure. Minimum entropy ($H = 0$ for a certain outcome) corresponds to complete order — one outcome determined, the full integer structure collapsed to a single value.

A concrete example clarifies the relationship. Consider a system with 9 outcomes whose probabilities follow the Benford distribution: $p_d = \log_{10}(1 + 1/d)$. This distribution has Shannon entropy $H_{\text{Ben}} = -\sum_{d=1}^{9} p_d \log_2 p_d \approx 2.88$ bits — less than the maximum $\log_2 9 \approx 3.17$ bits for a uniform distribution. The gap $H_{\max} - H_{\text{Ben}} \approx 0.29$ bits quantifies the order present in the Benford distribution — the structure inherited from the integer foundation. A system whose distribution matches Benford ($\delta_B = 0$) has this specific amount of structure; deviations from Benford shift $H$ in a direction that correlates with $\delta_B$. The conjecture that $H$ and $\delta_B$ are functionally related — that Shannon entropy is the logarithmic distance from order — remains to be derived formally, but the directional correspondence is clear: maximum $H$ corresponds to maximum $\delta_B$, and Benford conformance ($\delta_B = 0$) corresponds to a specific, submaximal $H$.

### 9.2 Landauer's Principle

Erasing a bit costs $kT \ln 2$ because erasure destroys order — it removes integer structure from the physical system. The energy cost is the thermodynamic price of undoing what the primes generated. One cannot erase order for free because the Second Law protects the integer structure: returning to Benford conformance (the natural state) requires dissipating the energy that was invested in creating the deviation.

The factor $\ln 2$ arises because a bit is the simplest order — a single binary ($p = 2$) constraint. Higher-order erasure (ternary, quaternary) would cost $kT \ln 3$, $kT \ln 5$, etc., corresponding to higher primes. The general erasure cost for a $p$-ary digit is $kT \ln p$.

### 9.3 Englert's Distinguishability

Englert's $D = \frac{1}{2} \|\rho_L - \rho_R\|_1$ measures how much integer structure the detector has captured from the system. Full distinguishability ($D = 1$) means the detector has absorbed the complete path information — the full order that was present in the superposition. Zero distinguishability ($D = 0$) means no order has been transferred; the system retains its full coherence.

The complementarity relation $D^2 + \mathcal{V}^2 \leq 1$ becomes a **conservation law for order**: the total integer structure is conserved between the interference pattern ($\mathcal{V}$, where order manifests as fringe contrast) and the detector ($D$, where order manifests as which-path knowledge). Order moves between subsystems but is never created or destroyed.

This reinterpretation predicts a quantitative relationship: $\delta_B$ computed from the experimental data should correlate with $D$ computed from the detector state. The conjecture $\delta_B \propto D$ (or a monotonic function thereof) is testable.

### 9.4 Wheeler Corrected

Wheeler was right that physical reality derives from something informational. But the unit is wrong. It is not "it from bit." It is **order from primes.** The bit — the binary distinction, the even/odd partition — is a consequence of the simplest prime ($p = 2$), not the foundation of reality. The foundation is the full set of primes; the bit is where the structure starts, not where it ends.

Wheeler's vision is preserved but grounded: reality derives from the integer structure generated by primes through multiplication, as expressed in the Euler product of the Riemann zeta function. "It from bit" becomes "order from primes."

---

## 10. Higher Primes and Decoherence

### 10.1 $p = 2$: The Binary Partition

The smallest prime generates the most fundamental partition: even vs. odd, left vs. right, spin-up vs. spin-down, boson vs. fermion. In the Euler product, the $p = 2$ factor controls the sign alternation between $\zeta(s)$ and $\eta(s)$:

$$\eta(s) = (1 - 2^{1-s}) \cdot \zeta(s)$$

The bridge factor $\varepsilon_B = |1 - 2^{1-s}|$ is the $p = 2$ contribution to the boson-fermion separation. In the double-slit experiment, the measurement asks a binary question — which slit? — activating the $p = 2$ factor specifically.

Binary information ($p = 2$) is the simplest order. It is the foundation of digital computing, Boolean logic, and the bit. But it is one prime among infinitely many.

### 10.2 $p = 3$ and the Triple-Slit Experiment: Information Still Undefined

The triple-slit experiment exposes the same definitional gap identified in Section 2 — but at the next level of complexity. In a three-slit setup, physicists speak of "three-path information" and compute generalized distinguishability measures for ternary alternatives. The Sorkin parameter $\kappa_3$ quantifies genuine three-way interference — the component of the interference pattern that cannot be decomposed into a sum of pairwise (two-slit) terms. Under Born's rule, $\kappa_3 = 0$ exactly: all interference is pairwise. Experimental tests (Sinha et al., 2010; Söllner et al., 2012) have confirmed $\kappa_3 = 0$ to high precision.

But what is "three-path information"? The same evasion from the double-slit case repeats: information is defined operationally (as a distinguishability measure over three detector states) without any physical referent. Shannon entropy handles ternary outcomes by switching to $\log_2 3$ bits, but this is a change of bookkeeping, not of substance. Englert's framework generalizes to three paths via the trace distance between three conditional density matrices, but the generalization says nothing about what *kind* of structure is being distinguished.

In the prime framework, the triple-slit experiment activates the $p = 3$ Euler factor $(1 - 3^{-s})^{-1}$. The double slit probes binary ($p = 2$) structure; the triple slit probes ternary ($p = 3$) structure — a genuinely different prime factor in the Euler product, not a repetition of $p = 2$. The Sorkin parameter $\kappa_3$ becomes the natural $p = 3$ analogue of the bridge factor $\varepsilon_B = |1 - 2^{1-s}|$: it measures how much irreducibly ternary structure is present.

This extends to higher primes:

| Prime | Partition | Experimental probe |
|-------|-----------|-------------------|
| $p = 2$ | Binary (even/odd) | Double slit, boson/fermion, spin-$\frac{1}{2}$ |
| $p = 3$ | Ternary | Triple slit, Sorkin parameter $\kappa_3$ |
| $p = 5$ | Quinary | Five-slit interference |
| All $p$ | Complete integer structure | Full physical reality |

The key point for this paper is not the specific physical identification of each prime — that remains open — but the structural observation: **existing definitions of information were built around binary distinctions.** Shannon's bit, Wheeler's bit, Englert's two-path $D$ — all are $p = 2$ constructs. The prime framework naturally accommodates all primes, providing a definition of information that is not artificially restricted to binary. A bit is where the integer structure starts; it is not where it ends.

### 10.3 Decoherence as Loss of Multiplicative Structure

Decoherence — the process by which quantum systems lose coherence through interaction with the environment — is standardly described as the suppression of off-diagonal elements in the density matrix. In the present framework, decoherence has a structural interpretation:

**Decoherence is the loss of multiplicative structure in the integer foundation.**

A fully coherent system has an intact Euler product: all primes contribute, and the multiplicative structure is complete. As the system interacts with its environment, individual prime factors are "pinned" — constrained by the interaction to specific values. Each pinned prime reduces the multiplicative freedom. The system's leading-digit distribution drifts away from Benford as the multiplicative structure degrades.

Full decoherence corresponds to all prime factors being constrained — no multiplicative freedom remains. The system is fully classical: definite values, definite properties, no superposition. The $\delta_B$ increase tracks the loss of prime-by-prime multiplicative freedom.

This picture predicts that decoherence rates should depend on the number of prime factors being constrained simultaneously. An interaction that pins only $p = 2$ (a binary measurement) causes less decoherence than one that pins $p = 2$ and $p = 3$ simultaneously. Multi-slit experiments with varying slit numbers could test this prediction directly.

---

## 11. Falsification

The proposal is falsifiable. The following observations would disprove it:

### 11.1 Primary Test

**If $\delta_B$ does not track quantum-to-classical transitions, the framework fails.** The core prediction is that systems with small $\delta_B$ exhibit quantum behavior (interference, entanglement, coherence) and systems with large $\delta_B$ exhibit classical behavior (definite values, decoherence, localization). If a system is found with $\delta_B \approx 0$ that behaves entirely classically, or a system with large $\delta_B$ that exhibits robust quantum coherence, the framework is falsified.

This test can be performed with existing technology. Measure the leading-digit distribution of physical quantities in any quantum-to-classical crossover experiment (BEC formation, decoherence in ion traps, superconductor transition). Compute $\delta_B$ at each stage. If $\delta_B$ does not correlate with the quantum-classical character of the system, the proposal fails.

### 11.2 Secondary Tests

1. **A Benford-conformant system with no quantum behavior.** If a macroscopic, fully classical system (no interference, no entanglement, no coherence) is found with $\delta_B = 0$ across all measurable quantities, then Benford conformance does not diagnose quantum character, and the framework is falsified.

2. **$\delta_B$ uncorrelated with $D$.** If Englert's distinguishability $D$ and Benford deviation $\delta_B$ are found to be uncorrelated in a double-slit experiment with variable which-path detection, the connection between order and measurement breaks down.

3. **Benford conformance insensitive to decoherence.** If the Benford deviation of a system does not change as decoherence is externally controlled (e.g., by varying the coupling to a thermal bath), then $\delta_B$ is not measuring the relevant physics.

### 11.3 What the Framework Does Not Predict

To avoid overreach, the following are explicitly *outside* the scope of this proposal:

- The values of coupling constants (fine structure constant, strong coupling, etc.)
- The specific masses of elementary particles
- The dimensionality of spacetime
- The matter content of the Standard Model

The proposal defines what information *is* and provides a diagnostic measure ($\delta_B$). It does not derive the specific information content of the universe — that would require a complete theory of initial conditions.

---

## 12. Discussion

### 12.1 The Black Hole Information Paradox

The black hole information paradox asks: when matter falls into a black hole and the black hole subsequently evaporates via Hawking radiation, is the quantum information of the infalling matter preserved?

In the present framework, the question dissolves. If information is the integer structure encoded in $\zeta(s)$, and the Euler product diverges at $s = 1$ (the horizon), then the integer structure itself breaks down at the horizon. Information is not "destroyed" — the mathematical framework in which information is *defined* ceases to be valid. Asking "where does the information go?" across the horizon is like asking "what is the temperature of a single molecule?" — it applies a concept outside its domain of definition.

This does not resolve the paradox in the sense of proving unitarity or non-unitarity. It reframes the question: the paradox arises from treating information as a substance that must be conserved across a boundary where the structure defining information (the Euler product) breaks down. Whether a modified definition of "information" valid at $s \leq 1$ exists is an open question.

### 12.2 Connections to Existing Programs

The proposal connects to several active research programs:

- **Sorkin's causal sets** (Paper #6). If primes are causal set elements, then information (integer structure) is the emergent order of the causal set. The partial order of the causal set maps to the divisibility structure of the integers.
- **Berry-Keating conjecture.** The conjecture that the Riemann zeros correspond to eigenvalues of a quantum Hamiltonian $H = xp$ is consistent with the present framework: the zeros of $\zeta(s)$ would be energy levels, and their distribution (governed by the prime structure) would encode physical information.
- **Hartnoll-Yang conformal primon gas** (2025). Their partition function $Z = \prod_p (1 - p^{-s})^{-1}$ — the Euler product itself — describes a gas of "primons," one for each prime. The present proposal interprets this gas physically: primons are the foundation; the partition function is the sum over all integer states (all information).
- **Godet (2025).** The connection between Möbius randomness and the Hartle-Hawking state suggests that the prime foundation has direct quantum gravitational content.

### 12.3 Limitations

Three aspects of the proposal require further development:

1. **The Landauer sketch.** The connection between $\ln 2$ and the $p = 2$ Euler factor (Section 7.2) is suggestive but not rigorous. A complete derivation of Landauer's principle from the prime framework would require connecting the Euler product to the canonical partition function.

2. **The $\delta_B = D$ conjecture.** The predicted correlation between Benford deviation and Englert's distinguishability (Section 9.3) has not been tested experimentally. The computational evidence (Section 8.1) shows the correlation exists in simulation ($r = +0.36$) but does not establish the functional form.

3. **Scope.** The proposal defines information and provides a diagnostic but does not derive specific physical constants or the matter content of the universe. It is a definition, not a Theory of Everything.

---

## 13. Conclusions

The main results of this paper are:

1. **Information has a physical referent.** Information is order — the structure of the integers, generated by the prime numbers through multiplication, as encoded in the Euler product of the Riemann zeta function.

2. **The measure is $\delta_B$.** The Benford deviation quantifies how faithfully a physical system reflects the integer-generated baseline. It is dimensionless, scale-invariant, and computable from any dataset of positive real numbers.

3. **Benford's Law is the signature.** The Benford distribution $P(d) = \log_{10}(1 + 1/d)$ is the least-action distribution under scale invariance — the statistical ground state of the integer structure.

4. **The metric encodes information.** The Schwarzschild metric built from $\zeta(s) \times 1/\zeta(s) = 1$ expresses the coexistence of order (integers/time) and foundation (primes/space) at every spacetime point.

5. **Entropy is the return to Benford.** The Second Law of Thermodynamics is the natural relaxation of $\delta_B \to 0$ — the system returning to the least-action distribution where no prime is favored.

6. **Existing definitions are consequences.** Shannon entropy measures logarithmic distance from order. Landauer's $kT \ln 2$ is the energy cost of one binary ($p = 2$) constraint. Englert's $D^2 + \mathcal{V}^2 \leq 1$ is conservation of order between system and detector.

7. **Wheeler corrected.** Not "it from bit" — "order from primes." The bit ($p = 2$) is where the integer structure starts, not where it ends.

8. **Higher primes generate deeper structure.** Binary distinctions ($p = 2$) are the simplest order. Triple-slit interference ($p = 3$), higher-order multi-path experiments, and the full complexity of physical reality involve all primes. Existing definitions of information are built around binary — the prime framework is not.

9. **Decoherence is loss of multiplicative structure.** As the environment pins individual prime factors, the Euler product loses degrees of freedom and $\delta_B$ increases.

10. **The proposal is falsifiable.** If $\delta_B$ does not track quantum-to-classical transitions in experimental systems, the framework fails.

---

## References

### Own Papers

[1] Riner, C. J. W. (2025). *Modified Schwarzschild Metric via Benford's Law.* Zenodo. DOI: 10.5281/zenodo.18553466. (Paper #1 in series.)

[2] Riner, C. J. W. (2025). *Bose-Einstein Condensates + Benford's Law.* Zenodo. DOI: 10.5281/zenodo.18510250. (Paper #2 in series.)

[3] Riner, C. J. W. (2025). *Prime Numbers as Causal Set Theory.* Zenodo. DOI: 10.5281/zenodo.18731508. (Paper #6 in series.)

[4] Riner, C. J. W. (2026). *Emergence of General Relativity from the Prime Number Structure of the Riemann Zeta Function.* Zenodo. DOI: 10.5281/zenodo.18751909. (Paper #7 in series.)

### Information Theory Foundations

[5] Shannon, C. E. (1948). A Mathematical Theory of Communication. *Bell System Technical Journal,* 27(3), 379–423.

[6] Von Neumann, J. (1932). *Mathematische Grundlagen der Quantenmechanik.* Springer, Berlin.

[7] Landauer, R. (1961). Irreversibility and Heat Generation in the Computing Process. *IBM Journal of Research and Development,* 5(3), 183–191.

[8] Wheeler, J. A. (1989). Information, physics, quantum: the search for links. *Proceedings of the 3rd International Symposium on Foundations of Quantum Mechanics,* 354–368.

[9] Englert, B.-G. (1996). Fringe Visibility and Which-Way Information: An Inequality. *Physical Review Letters,* 77(11), 2154–2157.

### Experimental Verification of Landauer's Principle

[10] Berut, A. et al. (2012). Experimental verification of Landauer's principle. *Nature,* 483, 187–189.

[11] Yan, L. L. et al. (2018). Single-atom demonstration of quantum Landauer principle. *Physical Review Letters,* 120, 210601.

### Benford's Law

[12] Newcomb, S. (1881). Note on the Frequency of Use of the Different Digits in Natural Numbers. *American Journal of Mathematics,* 4(1), 39–40.

[13] Benford, F. (1938). The Law of Anomalous Numbers. *Proceedings of the American Philosophical Society,* 78(4), 551–572.

[14] Burke, J. & Kincanon, E. (1991). Benford's law and physical constants. *American Journal of Physics,* 59(10), 952.

[15] Sen(De), A. & Sen, U. (2011). Benford's law detects quantum phase transitions similarly as earthquakes. *EPL,* 95, 10006.

[16] Rane, A., Mishra, A., Biswas, A., Sen(De), A. & Sen, U. (2014). Benford's law gives better scaling exponents in phase transitions. *Physical Review E,* 90, 022144.

[17] Bera, S., Mishra, A., Roy, S. S., Biswas, A. & De, A. S. (2018). Benford analysis of quantum critical phenomena. *Physics Letters A,* 382, 1639–1644.

[18] Cong, P., Li, M. & Ma, L. (2019). First digit law from Laplace transform. *Physics Letters A,* 383, 1836–1844.

[19] Weyl, H. (1916). Über die Gleichverteilung von Zahlen mod. Eins. *Mathematische Annalen,* 77, 313–352.

[20] Einsiedler, M. & Ward, T. (2011). *Ergodic Theory with a View Towards Number Theory.* Graduate Texts in Mathematics 259, Springer.

[21] Jaynes, E. T. (1957). Information Theory and Statistical Mechanics. *Physical Review,* 106(4), 620–630.

### Wave-Particle Duality and Quantum Measurement

[22] Scully, M. O., Englert, B.-G. & Walther, H. (1991). Quantum Optical Tests of Complementarity. *Nature,* 351, 111–116.

[23] Kim, Y.-H., Yu, R., Kulik, S. P., Shih, Y. & Scully, M. O. (2000). Delayed 'Choice' Quantum Eraser. *Physical Review Letters,* 84, 1–5.

[24] Walborn, S. P., Terra Cunha, M. O., Padua, S. & Monken, C. H. (2002). Double-Slit Quantum Eraser. *Physical Review A,* 65, 033818.

[25] Coles, P. J., Kaniewski, J. & Wehner, S. (2014). Equivalence of wave-particle duality to entropic uncertainty. *Nature Communications,* 5, 5814.

[26] Batelaan, H., Jones, E., Huang, W. C.-W. & Bach, R. (2020). Momentum exchange in the electron double-slit experiment. arXiv: 2012.02141.

### Triple-Slit Experiments

[27] Sinha, U., Couteau, C., Jennewein, T., Laflamme, R. & Weihs, G. (2010). Ruling Out Multi-Order Interference in Quantum Mechanics. *Science,* 329(5990), 418–421.

[28] Söllner, I., Gschösser, B., Mai, P., Preber, B., Vörös, Z. & Weihs, G. (2012). Testing Born's rule in quantum mechanics for three mutually exclusive events. *Foundations of Physics,* 42, 742–751.

### Interaction-Free Measurements

[29] Elitzur, A. C. & Vaidman, L. (1993). Quantum Mechanical Interaction-Free Measurements. *Foundations of Physics,* 23, 987–997.

[30] Kwiat, P. G. et al. (1999). High-Efficiency Quantum Interrogation via Quantum Zeno Effect. *Physical Review Letters,* 83, 4725.

[31] Robens, C., Alt, W., Emary, C., Meschede, D. & Alberti, A. (2017). Atomic Bomb Testing: Elitzur-Vaidman Violates Leggett-Garg. *Applied Physics B,* 123, 12.

### Quantum Measurement Thermodynamics

[32] Elouard, C., Herrera-Marti, D., Clusel, M. & Auffeves, A. (2016). The role of quantum measurement in stochastic thermodynamics. *npj Quantum Information,* 3, 9.

[33] Jordan, A. N., Elouard, C. & Auffeves, A. (2019). Quantum measurement engines and their relevance for quantum interpretations. arXiv: 1911.06838.

[34] Kammerlander, P. & Anders, J. (2015). Coherence and measurement in quantum thermodynamics. *Scientific Reports,* 6, 22174.

[35] Latune, C. & Elouard, C. (2024). A thermodynamically consistent approach to the energy costs of quantum measurements. *Quantum,* 8, 1526.

### Riemann Zeta Function in Physics

[36] Berry, M. V. & Keating, J. P. (1999). The Riemann Zeros and Eigenvalue Asymptotics. *SIAM Review,* 41(2), 236–266.

[37] Sierra, G. & Townsend, P. K. (2008). Landau levels and Riemann zeros. *Physical Review Letters,* 101, 110201.

[38] LeClair, A. & Mussardo, G. (2023). Riemann zeros as quantized energies of scattering with impurities. *JHEP,* 2023, 62.

[39] Hartnoll, S. A. & Yang, E. (2025). The conformal primon gas at the end of time. *JHEP,* 2025, 34.

[40] Godet, V. (2025). Mobius randomness in the Hartle-Hawking state. arXiv: 2505.03068.

[41] Tamburini, F. (2025). Majorana particle spectrum in Rindler spacetime encoded by zeta. arXiv: 2503.09644.

[42] Yakaboylu, E. (2024). Hamiltonian for the Hilbert-Polya conjecture. *Journal of Physics A,* 57, 235203.

[43] Kalauni, P. & Milton, K. A. (2023). Supersymmetric quantum mechanics and the Riemann hypothesis. arXiv: 2211.04382.

### Causal Set Theory and Quantum Gravity

[44] Sorkin, R. D. (2003). Causal sets: Discrete gravity. In *Lectures on Quantum Gravity,* Springer, 305–327.

[45] Modesto, L. (2004). Disappearance of black hole singularity in quantum gravity. *Physical Review D,* 70, 124009.

[46] Bonanno, A. & Reuter, M. (2000). Renormalization group improved black hole spacetimes. *Physical Review D,* 62, 043008.

[47] Nicolini, P., Smailagic, A. & Spallucci, E. (2006). Noncommutative geometry inspired Schwarzschild black hole. *Physics Letters B,* 632, 547–551.

### Black Hole Information

[48] Bekenstein, J. D. (1973). Black holes and entropy. *Physical Review D,* 7(8), 2333–2346.

[49] Hawking, S. W. (1975). Particle creation by black holes. *Communications in Mathematical Physics,* 43(3), 199–220.

[50] 't Hooft, G. (1993). Dimensional reduction in quantum gravity. arXiv: gr-qc/9310026.

[51] Susskind, L. (1995). The world as a hologram. *Journal of Mathematical Physics,* 36(11), 6377–6396.

---

## Appendix A. Simulation Parameters and Code Availability

All simulations referenced in this paper are implemented in Python 3 and available in the public repository:

**Repository:** [github.com/jackwayne234/research-hub](https://github.com/jackwayne234/research-hub)

### Table 1 — BEC Benford deviation (Section 8.1)

**Script:** `analysis/kretschner/double_slit_test.py`

| Parameter | Value |
|-----------|-------|
| Energy levels | 5,000 |
| Temperature range | $T \in \{0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 50.0, 100.0\}$ |
| Bosonic distribution | Bose-Einstein: $\bar{n}_i = 1/(e^{E_i/T} - 1)$ |
| Fermionic distribution | Fermi-Dirac: $\bar{n}_i = 1/(e^{E_i/T} + 1)$ |
| $\delta_B$ convention | Sum of absolute deviations ($9 \times$ the mean definition in Section 4.4) |

### Table 2 — Coupling simulation (Section 8.1)

**Script:** `analysis/kretschner/double_slit_test.py`

| Parameter | Value |
|-----------|-------|
| Bosonic modes | 3,000 |
| Fermionic modes | Up to 3,000 (scaled by coupling $\alpha$) |
| Coupling range | $\alpha \in \{0.00, 0.05, 0.10, 0.20, 0.30, 0.50, 1.00\}$ |
| Temperature | $T = 1.0$ (natural units) |
| Distinguishability | $D = 2\alpha$ (linear model) |
| Visibility | $\mathcal{V} = \sqrt{1 - D^2}$ (Englert relation) |

### Tables 3 & 4 — Kretschmann scalar (Sections 8.2, 8.3)

**Scripts:** `analysis/kretschner/kretschner_scalar.py` (single model), `analysis/kretschner/nine_models_kretschner.py` (nine QG models)

| Parameter | Value |
|-----------|-------|
| Schwarzschild radius | $r_s = 1$ (normalized) |
| Planck length | $r_P = 10^{-4} \, r_s$ (exaggerated for visibility; realistic: $\sim 10^{-38} \, r_s$) |
| Radial mapping | $s(r) = 1 + (r/r_s)^3$ |
| Zeta precision | 30 decimal places (mpmath library) |
| $K$ computation | Central-difference numerical derivatives of $f(r)$ |
| Radial points | $r/r_s \in \{10, 2, 1.1, 1.01, 1.001, 0.99, 0.5, 0.3, 0.1, 0.04, 0.01\}$ |

### Dependencies

- Python 3.8+
- `mpmath` (arbitrary-precision arithmetic, required for zeta evaluation near $s = 1$)
- No other external dependencies

### Reproducing the results

```
git clone https://github.com/jackwayne234/research-hub.git
cd research-hub/analysis/kretschner
pip install mpmath
python3 double_slit_test.py      # Tables 1, 2
python3 kretschner_scalar.py     # Table 3
python3 nine_models_kretschner.py  # Table 4
```

All scripts print results to stdout. No random seeds are used in the Kretschmann calculations (deterministic). The BEC simulation uses deterministic occupation number formulas; leading-digit extraction is exact.

---

*Correspondence: chrisriner45@gmail.com*
*ORCID: 0009-0008-9448-9033*
